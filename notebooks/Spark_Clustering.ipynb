{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/bro.png\" width=\"100px\"></div>\n",
    "\n",
    "# Bro to Spark: Clustering\n",
    "In this notebook we will pull Bro data into Spark then do some analysis and clustering. The first step is to convert your Bro log data into a Parquet file, for instructions on how to do this (just a few lines of Python code using the BAT package) please see this notebook:\n",
    "\n",
    "<div style=\"float: right; margin: 0px 0px 0px 0px\"><img src=\"images/parquet.png\" width=\"300px\"></div>\n",
    "\n",
    "### Bro logs to Parquet Notebook\n",
    "- [Bro to Parquet to Spark](https://github.com/Kitware/bat/blob/master/notebooks/Bro_to_Parquet_to_Spark.ipynb)\n",
    "\n",
    "Apache Parquet is a columnar storage format focused on performance. Parquet data is often used within the Hadoop ecosystem and we will specifically be using it for loading data into Spark.\n",
    "\n",
    "<div style=\"float: right; margin: 0px 0px 0px 0px\"><img src=\"images/mllib.png\" width=\"200px\"></div>\n",
    "<div style=\"float: right; margin: 30px 0px 0px 0px\"><img src=\"images/spark.png\" width=\"200px\"></div>\n",
    "\n",
    "### Software\n",
    "- Bro Analysis Tools (BAT): https://github.com/Kitware/bat\n",
    "- Parquet: https://parquet.apache.org\n",
    "- Spark: https://spark.apache.org\n",
    "- Spark MLLib: https://spark.apache.org/mllib/\n",
    "\n",
    "### Data\n",
    "- Sec Repo: http://www.secrepo.com (no headers on these)\n",
    "- Kitware: [data.kitware.com](https://data.kitware.com/#collection/58d564478d777f0aef5d893a) (with headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAT: 0.2.9\n",
      "PySpark: 2.2.0\n",
      "PyArrow: 0.6.0\n"
     ]
    }
   ],
   "source": [
    "# Third Party Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyarrow\n",
    "\n",
    "# Local imports\n",
    "import bat\n",
    "from bat.log_to_parquet import log_to_parquet\n",
    "\n",
    "# Good to print out versions of stuff\n",
    "print('BAT: {:s}'.format(bat.__version__))\n",
    "print('PySpark: {:s}'.format(pyspark.__version__))\n",
    "print('PyArrow: {:s}'.format(pyarrow.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/spark.png\" width=\"200px\"></div>\n",
    "\n",
    "# Spark It!\n",
    "### Spin up Spark with 4 Parallel Executors\n",
    "Here we're spinning up a local spark server with 4 parallel executors, although this might seem a bit silly since we're probably running this on a laptop, there are a couple of important observations:\n",
    "\n",
    "<div style=\"float: right; margin: 20px 20px 20px 20px\"><img src=\"images/spark_jobs.png\" width=\"400px\"></div>\n",
    "\n",
    "- If you have 4/8 cores use them!\n",
    "- It's the exact same code logic as if we were running on a distributed cluster.\n",
    "- We run the same code on **DataBricks** (www.databricks.com) which is awesome BTW.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spin up a local Spark Session (with 4 executors!)\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName('my_awesome').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 50px 20px 20px -20px\"><img src=\"images/parquet.png\" width=\"350px\"></div>\n",
    "##  Read in our Parquet File\n",
    "Here we're loading in a Bro HTTP log with ~2 million rows to demonstrate the functionality and do some analysis and clustering on the data. For more information on converting Bro logs to Parquet files please see our Bro to Parquet notebook:\n",
    "\n",
    "#### Bro logs to Parquet Notebook\n",
    "- [Bro to Parquet to Spark](https://github.com/Kitware/bat/blob/master/notebooks/Bro_to_Parquet_to_Spark.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Have Spark read in the Parquet File\n",
    "spark_df = spark.read.parquet(\"http.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: left; margin: 20px 20px 20px 20px\"><img src=\"images/eyeball.jpeg\" width=\"150px\"></div>\n",
    "# Lets look at our data\n",
    "We should always inspect out data when it comes in. Look at both the data values and the data types to make sure you're getting exactly what you should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 2048442\n",
      "Columns: filename,host,id.orig_h,id.orig_p,id.resp_h,id.resp_p,info_code,info_msg,method,orig_fuids,orig_mime_types,password,proxied,referrer,request_body_len,resp_fuids,resp_mime_types,response_body_len,status_code,status_msg,tags,trans_depth,uid,uri,user_agent,username,ts\n"
     ]
    }
   ],
   "source": [
    "# Get information about the Spark DataFrame\n",
    "num_rows = spark_df.count()\n",
    "print(\"Number of Rows: {:d}\".format(num_rows))\n",
    "columns = spark_df.columns\n",
    "print(\"Columns: {:s}\".format(','.join(columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "| method|status_code|  count|\n",
      "+-------+-----------+-------+\n",
      "|   HEAD|        404|1294022|\n",
      "|    GET|        404| 429361|\n",
      "|   POST|        200| 125638|\n",
      "|    GET|        200|  88636|\n",
      "|   POST|          0|  32918|\n",
      "|    GET|        400|  29152|\n",
      "|    GET|        303|  10858|\n",
      "|    GET|        403|   8530|\n",
      "|   POST|        404|   4277|\n",
      "|    GET|        304|   3851|\n",
      "|    GET|        302|   3250|\n",
      "|    GET|          0|   2823|\n",
      "|    GET|        401|   2159|\n",
      "|OPTIONS|        200|   1897|\n",
      "|   POST|        302|   1226|\n",
      "|   HEAD|        503|   1010|\n",
      "|   POST|        206|    869|\n",
      "|    GET|        301|    642|\n",
      "|   HEAD|          0|    606|\n",
      "|    GET|        503|    550|\n",
      "+-------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.groupby('method','status_code').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 50px 0px 0px 20px\"><img src=\"images/deep_dive.jpeg\" width=\"350px\"></div>\n",
    "\n",
    "# Data looks good, lets take a deeper dive\n",
    "Spark has a powerful SQL engine as well as a Machine Learning library. So now that we've loaded our Bro data we're going to utilize the Spark SQL commands to do some investigation of our data including clustering from the MLLib.\n",
    "\n",
    "<div style=\"float: left; margin: 20px 0px 0px 0px\"><img src=\"images/spark_sql.jpg\" width=\"180px\"></div>\n",
    "<div style=\"float: left; margin: 0px 50px 0px 0px\"><img src=\"images/mllib.png\" width=\"180px\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Trains a k-means model.\n",
    "    kmeans = KMeans().setK(2).setSeed(1)\n",
    "    model = kmeans.fit(dataset)\n",
    "\n",
    "    # Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "    wssse = model.computeCost(dataset)\n",
    "    print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "    # Shows the result.\n",
    "    centers = model.clusterCenters()\n",
    "    print(\"Cluster Centers: \")\n",
    "    for center in centers:\n",
    "        print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Coming..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 50px 0px 0px -20px\"><img src=\"https://www.kitware.com/img/small_logo_over.png\" width=\"250px\"></div>\n",
    "## Wrap Up\n",
    "Well that's it for this notebook, we pulled in Bro data from a Parquet file, then did some digging with high speed, parallel SQL operations and we clustered our data to organize the restuls.\n",
    "\n",
    "If you liked this notebook please visit the [BAT](https://github.com/Kitware/bat) project for more notebooks and examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
